### Generic
1. https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/
2. Hongyi Lee: [https://youtu.be/n9TlOhRjYoc](https://youtu.be/n9TlOhRjYoc)
3. This video: [https://youtu.be/C4jmYHLLG3A?list=PLam9sigHPGwOBuH4_4fr-XvDbe5uneaf6](https://youtu.be/C4jmYHLLG3A?list=PLam9sigHPGwOBuH4_4fr-XvDbe5uneaf6)
4. Original post: [https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)
5. Good reference, but too long: [https://e2eml.school/transformers.html](https://e2eml.school/transformers.html)
6. [nanoGPT/transformer_sizing.ipynb at master · karpathy/nanoGPT · GitHub](https://github.com/karpathy/nanoGPT/blob/master/transformer_sizing.ipynb)
7. Nice series from [CodeEmporium](https://youtu.be/QCJQG4DuHT0?si=cZnMAHEQuNZWe4J-)


### Positional embedding
1. https://kazemnejad.com/blog/transformer_architecture_positional_encoding/

### LayerNorm
1. it's done at the *layer/embedding* dimension; optionally the batch dimension
2. Nice clear example https://youtu.be/G45TuC6zRf4
3. https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html