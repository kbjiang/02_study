Date: 2025-02-06
Date of publish: 
Authors: Andrej Karpathy
Tags: #LLM 

Talk link: https://youtu.be/7xTGNNLPyMI
Paper link:
Related: 

## Main results
1. Very nice explanation on SFT to RL, with the example of learning chemistry
	1. pretraining: exposition to background knowledge, i.e., texts in the textbook
	2. SFT: imitation of human expert, i.e., examples in the textbook; blind mimicry.
	3. RL: reward only, no intermediates provided, trial and error. i.e., practice problems with check on final answer.
2. Very nice explanation on why we cannot predefine the learning paths for LLMs (by SFT/imitation) and RL is indispensable
	1. Via RL, LLM overtime figures out the paths best suit itself

## What I don't agree

## Questions
